{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"mount_file_id":"1V9FjFv6K-QHxSpKrxNn6elc6zkhX5cPu","authorship_tag":"ABX9TyO0j2hwbPW/tkrjYIAKRHv4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"f1e92cacf54742ad8256c6191b8bcc6c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_229d713cf2f842e48d2cf0e1662fd0ff","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_57a268e19c04405b88f805de15a73b33","IPY_MODEL_0b8e0db4a22343019e052d3c4a4b80b9"]}},"229d713cf2f842e48d2cf0e1662fd0ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"57a268e19c04405b88f805de15a73b33":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b8ae754cbb4041b5ae15d561de57c108","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":102502400,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":102502400,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f01e8e92edcd406f95e367a1fe0ed3da"}},"0b8e0db4a22343019e052d3c4a4b80b9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ac0020ed4a73405d90c624d282b7bfc1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 97.8M/97.8M [00:01&lt;00:00, 82.0MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_755405fbc7b2415eafd2bb756f8b9d69"}},"b8ae754cbb4041b5ae15d561de57c108":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f01e8e92edcd406f95e367a1fe0ed3da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ac0020ed4a73405d90c624d282b7bfc1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"755405fbc7b2415eafd2bb756f8b9d69":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["f1e92cacf54742ad8256c6191b8bcc6c","229d713cf2f842e48d2cf0e1662fd0ff","57a268e19c04405b88f805de15a73b33","0b8e0db4a22343019e052d3c4a4b80b9","b8ae754cbb4041b5ae15d561de57c108","f01e8e92edcd406f95e367a1fe0ed3da","ac0020ed4a73405d90c624d282b7bfc1","755405fbc7b2415eafd2bb756f8b9d69"]},"id":"1T-_R96CYDTe","executionInfo":{"status":"error","timestamp":1612447891995,"user_tz":-60,"elapsed":29131,"user":{"displayName":"VAHID FARJOOD CHAFI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1qlhJVh2Na0BWeA4HjbsGIKblq2p5iko0FwkM=s64","userId":"03819353067086790089"}},"outputId":"6d6f9573-5765-4e24-ab33-2769bdcd7aba"},"source":["import os\n","import spacy\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","from collections import Counter\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import argparse\n","import random\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","import torchvision.models as models\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.nn.utils.rnn import pack_padded_sequence\n","from torch.utils.data import DataLoader, Dataset\n","\n","\n","class TextManager:\n","    \"\"\"Class that will build vocabulary and tokenization utilities to be used by natural network model\"\"\"\n","\n","    def __init__(self, freq_threshold=None):\n","        \"\"\"Initialization:\n","\n","        Args:\n","            freq_threshold: specify the threshold for frequency of a word to be added in vocabulary \n","        \"\"\"\n","\n","        # create the vocabulary with some pre-reserved tokens: \n","        self.vocabulary = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n","        #PAD: padding , SOS: start of sentence , EOS: end of sentence , UNK: unknown word\n","\n","        self.spacy_en = spacy.load(\"en\") # for a better text tokenization purpose\n","        self.freq_threshold = freq_threshold# threshold of word frequency\n","\n","\n","    def __len__(self):\n","        \"\"\"The total number of words in the vocabulary.\"\"\"\n","\n","        return len(self.vocabulary)\n","\n","\n","    def get_key(self, val):\n","        \"\"\"Method to be used to get the key of a value in vocabulary\n","        Args:\n","            val: the index of the corresponding word in vocabulary\n","\n","        Return: \n","            key: the word corresponding to the given index of the vocabulary\n","        \"\"\"\n","\n","        for key, value in self.vocabulary.items():\n","            if val == value:\n","                return key\n","\n","    def tokenize(self, text):\n","        \"\"\"Method for text tokenization (using spacy)\"\"\"\n","\n","        return [token.text.lower() for token in self.spacy_en.tokenizer(text)]\n","\n","    def build_vocab(self, sentence_list):\n","        \"\"\"Method used to build the vocabulary \"\"\"\n","\n","        frequencies = Counter() #using 'Counter()' for counting the words\n","        idx = 4 #index of the first word of the sentence(0,1,2,3 are reserved!)\n","\n","        for sentence in sentence_list:\n","            for word in self.tokenize(sentence):\n","                frequencies[word] += 1\n","\n","                # add the word to the vocab if it reaches minimum frequency threshold\n","                if frequencies[word] == self.freq_threshold:\n","                    self.vocabulary[word] = idx\n","                    idx += 1\n","\n","    def numericalize(self, text):\n","        \"\"\"Method used for converting tokenized text into indices (using spacy)\n","            Args:\n","                text: input list of tokenized word\n","\n","            Return: a list of indices corresponding to each word\n","\n","        \"\"\"\n","        tokenized_text = self.tokenize(text)\n","        return [self.vocabulary[token] if token in self.vocabulary else self.vocabulary[\"<UNK>\"] for token in tokenized_text]\n","        \n","    def save(self, file):\n","        \"\"\"Save the vocabulary to file 'vocabulary.dat' \"\"\"\n","\n","        with open(file, 'w+', encoding='utf-8') as f:\n","            for word, idx in self.vocabulary.items():\n","                f.write(\"{} {}\\n\".format(idx, word))\n","\n","    def load(self, file):\n","        \"\"\"Load the vocabulary file 'vocabulary.dat' \"\"\"\n","\n","        self.vocabulary = {}\n","\n","        with open(file, 'r', encoding='utf-8') as f:\n","            for line in f:\n","                line_fields = line.split()\n","                self.vocabulary[line_fields[1]] = int(line_fields[0])\n","\n","    def load_embedding_file(self, embed_file):\n","        \"\"\" Creates an embedding matrix for the vocabulary.\n","        Args:\n","            embed_file: embeddings file with GloVe format\n","        :return: \n","            embeddings tensor in the same order as the words in the vocabulary\n","\n","        \"\"\"\n","\n","        if _data_set.vocab.vocabulary is None:\n","            raise ValueError(\"Vocabulary doesn't exists!!!\")\n","\n","        # Find embedding dimension\n","        with open(embed_file, 'r') as f:\n","            embed_size = len(f.readline().split()) - 1\n","    \n","        words_in_vocab = set(_data_set.vocab.vocabulary.keys()) # create a copy of words from vocabulary into a set\n","    \n","        # Create the initialized tensor for embeddings\n","        embedding_matrix = torch.FloatTensor(len(words_in_vocab), embed_size)\n","\n","        std = np.sqrt(5.0 / embedding_matrix.size(1))# used uniform distribution specially for pre-reserved tokens\n","        torch.nn.init.uniform_(embedding_matrix, -std, std)\n","\n","        for line in open(embed_file, 'r'):\n","            line_fields = line.split()\n","    \n","            embed_word = line_fields[0].lower() #force words to be in lower case\n","\n","            if embed_word not in words_in_vocab: # Ignoring the word which does not exists in the vocabulary\n","                continue\n","\n","            word_id = _data_set.vocab.vocabulary[embed_word]\n","            embedding_matrix[word_id, :] = torch.tensor([float(i) for i in line_fields[1:]], dtype=torch.float32)\n","    \n","        return embedding_matrix, embed_size\n","\n","\n","class CapDataset(Dataset):\n","    \"\"\" Image Captioning Dataset Class which makes a generic dataset for images and captions\"\"\"\n","\n","    def __init__(self, path, captions_file, preprocess_custom=None, empty_dataset=None, eval=None, test=None):\n","        \"\"\" create a dataset:\n","        Args:\n","            path: this is a string to the address of dataset directory\n","            captions_file: this is a string indication the name of captions file\n","            preprocess_custom: bool value indicating if we want a custom preprocess(True) or a default preprocess(None)\n","            empty_dataset: bool value indication if we are going to create an empty dataset or not\n","            eval: bool value indicating if we want to create a dataset for evaluation\n","            test: bool value indicating if we want to create a dataset for testing\n","        Returns:\n","\n","        \"\"\"\n","\n","        #initialize attributes\n","        self.root_dir = path #path to the root directory of dataset\n","        self.images = [] #holding the images loading from files\n","        self.captions = [] #holding the captions loading from files\n","\n","        self.preprocess = None #type of preprocessing for images\n","        self.data_mean = torch.zeros(3) #create a zeros tensor for mean\n","        self.data_std = torch.ones(3) #create a ones tensor for standard deviation\n","        self.data_mean[:] = torch.tensor([0.485, 0.456, 0.406]) #mean which is used by resnet50\n","        self.data_std[:] = torch.tensor([0.229, 0.224, 0.225]) #std which is used by resnet50\n","        \n","        #check whether the path is correct or not\n","        if path is None:\n","            raise ValueError(\"You must specify the dataset path!\")\n","        if not os.path.exists(path) or os.path.isfile(path):\n","            raise ValueError(\"Invalid data path: \" + str(path))\n","\n","        #check whether we want a custom preprocessing or just use a default operation\n","        if preprocess_custom is not None: #using a custom preprocess\n","            self.preprocess = T.Compose([\n","                T.RandomResizedCrop(224, scale=(0.08, 1.0), ratio=(3./4., 4./3.)),\n","                T.RandomHorizontalFlip(p=0.5),\n","                T.ToTensor(),\n","                T.Normalize(mean=self.data_mean, std=self.data_std),\n","                ])\n","        #using default preprocess\n","        else: \n","            self.preprocess = T.Compose([\n","                T.Resize(226),\n","                T.RandomCrop(224),\n","                T.ToTensor(),\n","                T.Normalize(mean=self.data_mean, std=self.data_std)\n","                ])\n","\n","\n","        #create a dataset for training purpose\n","        if not eval and not test:\n","            if not empty_dataset:\n","                # Get image and caption colum from the dataframe\n","                self.df = pd.read_csv(captions_file)\n","                self.files = self.df.values.tolist()\n","                self.images = (self.df[\"image\"]).values.tolist()\n","                self.captions = (self.df[\"caption\"]).values.tolist()\n","                # Initialize vocabulary object and build vocab\n","                self.vocab = TextManager(args.freq_threshold)\n","                self.vocab.build_vocab(self.captions)\n","                self.vocab.save('vocabulary.dat')\n","                print(\"Vocabulary size: \", len(self.vocab))\n","            # if we want to create an empty dataset(for splitting purpose)\n","            else: \n","                self.vocab = _data_set.vocab\n","                self.files = _data_set.files\n","\n","        # create a dataset in which you only want to evaluate a pre_trained model\n","        elif eval: \n","            if not os.path.exists('vocabulary.dat'):\n","                raise ValueError(\"There is no 'Vocabulary.dat' file\")\n","            self.df = pd.read_csv(captions_file)\n","            self.files = self.df.values.tolist()\n","            self.images = (self.df[\"image\"]).values.tolist()\n","            self.captions = (self.df[\"caption\"]).values.tolist()\n","            self.vocab = TextManager()\n","            self.vocab.load('vocabulary.dat')\n","            print(\"vocabulary is loaded!!  size=\", len(self.vocab))\n","\n","        # create a dataset in which you only want to test a pre_trained model\n","        elif test:\n","            if not os.path.exists('vocabulary.dat'):\n","                raise ValueError(\"There is no 'Vocabulary.dat' file\")\n","            folder_contents = os.listdir(self.root_dir)\n","            self.files = [f for f in folder_contents \n","                                if os.path.isfile(os.path.join(self.root_dir, f)) and f.endswith(\".jpg\")]\n","            for i in range(0, len(self.files)):\n","                self.images.append((self.files[i]))\n","                self.captions.extend([\"dummy dummy dummy dummy\"])\n","            self.vocab = TextManager()\n","            self.vocab.load('vocabulary.dat')\n","            print(\"vocabulary is loaded!!  size=\", len(self.vocab))\n","\n","\n","    def __len__(self):\n","        \"\"\"Compute the lenght of the data set(each image has 5 captions so each iamge is repeated 5 times)\"\"\"\n","        return len(self.captions)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"Load the next (image,caption) from disk.\n","        Args:\n","            index: the index of the element to be loaded.\n","        Returns:\n","            The image, caption, and caption lenght.\n","        \"\"\"\n","\n","        caption = self.captions[idx]\n","        img_name = self.images[idx]\n","        img_location = os.path.join(self.root_dir, img_name)\n","        img = Image.open(img_location).convert(\"RGB\")\n","\n","        #apply the transfromation to the image\n","        img = self.preprocess(img)\n","\n","        #numericalize the caption text\n","        caption_vec = []\n","        #add the SOS(start of a sentence)token at the begining of sentence\n","        caption_vec += [self.vocab.vocabulary[\"<SOS>\"]] \n","        caption_vec += self.vocab.numericalize(caption)\n","        #add the EOS(end of a sentence)token at the begining of sentence\n","        caption_vec += [self.vocab.vocabulary[\"<EOS>\"]] \n","        caption_len = torch.LongTensor([len(caption_vec)])\n","\n","        return img, torch.tensor(caption_vec), caption_len\n","\n","    def randomize_data(self):\n","        \"\"\"Randomize the dataset.\"\"\"\n","\n","        order = [a for a in range(len(self.files))]\n","        random.shuffle(order)\n","        # shuffling the data\n","        self.files = [self.files[a] for a in order]\n","\n","    def create_splits(self, proportions: list):\n","        \"\"\" split the dataset into our customize proportion\n","\n","            Args:\n","                proportions: a list in which we are going to split the dataset\n","\n","            returns:\n","                it returns a list of Dataset Objects(one Dataset per split)\n","\n","        \"\"\"\n","        p = 0.0\n","        invalid_prop_found = False\n","\n","        for prop in proportions:\n","            if prop <= 0.0:\n","                invalid_prop_found = True\n","                break\n","\n","            p +=prop\n","        if invalid_prop_found or p > 1.0 or len(proportions) == 0: # you are allowed to choose a portion of the data(good!!)\n","            raise ValueError(\"Invalid fraction for splitting!!!(It must be possitive and its sum must not be grater than 1)\")\n","\n","        data_size = len(self.files)\n","        num_splits = len(proportions)\n","        datasets = []\n","        for i in range(0, num_splits):\n","            if i==0:\n","                datasets.append(CapDataset(self.root_dir, self.files, preprocess_custom=args.preprocess, empty_dataset=True))\n","            elif i==1:\n","                datasets.append(CapDataset(self.root_dir, self.files, preprocess_custom=None, empty_dataset=True))\n","            else:\n","                datasets.append(CapDataset(self.root_dir, self.files, preprocess_custom=None, empty_dataset=True))\n","\n","        start = 0\n","        for i in range(0, num_splits):\n","            p = proportions[i]\n","            n = int(p * data_size)\n","            end = start + n \n","\n","            datasets[i].images.extend([self.images[z]for z in range(start, end)])\n","            datasets[i].captions.extend([self.captions[z]for z in range(start, end)])\n","            start = end\n","\n","        print(\"Dataset size:\", data_size)\n","        print(\"Selected Dataset size:\", len(datasets[0]) + len(datasets[1]) + len(datasets[2]) )\n","        print(\"-Training size:\", len(datasets[0]))\n","        print(\"-Validation size:\", len(datasets[1]))\n","        print(\"-Test size:\", len(datasets[2]))\n","        print(\"-------------------------------\")\n","\n","        return datasets\n","\n","    def CapCollate(self, batch):\n","        \"\"\"Collate to apply the padding to the captions with dataloader\n","        Args:\n","            batch: compose of images, caption, captions lenght \n","        Returns:\n","            images, padded captions, and captions lenght\n","        \"\"\"\n","        self.pad_idx = self.vocab.vocabulary[\"<PAD>\"]\n","    \n","        imgs = [item[0].unsqueeze(0) for item in batch] #get the images of the batch \n","        imgs = torch.cat(imgs, dim=0)\n","    \n","        targets = [item[1] for item in batch] #get the captions of the batch\n","        targets = pad_sequence(targets, batch_first=True, padding_value=self.pad_idx)\n","    \n","        caption_len = torch.LongTensor([item[2].item() for item in batch]) #create a tensor of captions lenght\n","    \n","        return imgs, targets, caption_len\n","\n","class Encoder_CNN(nn.Module):\n","    \"\"\"Class that models the CNN for resnet50 in order to encode the input images\"\"\"\n","\n","    def __init__(self):\n","        super(Encoder_CNN, self).__init__()\n","\n","        self.resnet = torch.hub.load('pytorch/vision:v0.7.0', 'resnet50', pretrained=True)\n","        for param in self.resnet.parameters():\n","            param.requires_grad = False\n","\n","        module = list(self.resnet.children())[:-2] #removing the last: AdaptiveAvgPool2d(), Linear(in=2048, out=1000)\n","        self.resnet = nn.Sequential(*module)\n","\n","    def forward(self, images):\n","\n","        features = self.resnet(images)  # (batch_size,2048,7,7)  Number Of feature Maps=2048, each feature size=7x7\n","        features = features.permute(0, 2, 3, 1)  # (batch_size,7,7,2048)\n","        features = features.view(features.size(0), -1, features.size(-1))  # (batch_size,49,2048) # .view(-1) inferred size\n","\n","        return features\n","\n","class Attention(nn.Module):\n","    \"\"\"Class that models the Attention neural model \"\"\"\n","    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n","        super(Attention, self).__init__()\n","\n","        #Linear transformation from decoder and encoder to attention dimension\n","        self.LinearEncoder = nn.Linear(encoder_dim, attention_dim)\n","        self.LinearDecoder = nn.Linear(decoder_dim, attention_dim) \n","\n","        # final projections\n","        self.relu = nn.ReLU()\n","        self.LinearAttention = nn.Linear(attention_dim, 1)\n","        \n","    def forward(self, features, hidden_state):\n","\n","        #linear transformation from encoder(CNN) and decoder(LSTM) dimension to attention dimension\n","        LinearEncoder_outputs = self.LinearEncoder(features)\n","        LinearDecoder_outputs = self.LinearDecoder(hidden_state) \n","\n","        #combine encoder and decoder outputs together (we can also use tanh())\n","        combined_output = self.relu(LinearEncoder_outputs + LinearDecoder_outputs.unsqueeze(1))\n","\n","        #compute the outpout of attention network\n","        attention_outputs = self.LinearAttention(combined_output)\n","\n","        #compute the alphas\n","        attention_scores = attention_outputs.squeeze(2) \n","        alphas = F.softmax(attention_scores, dim=1) #the sum of all alphas should be equal to one.\n","\n","        #compoute the context vector\n","        context_vector = features * alphas.unsqueeze(2)\n","        context_vector = context_vector.sum(dim=1)\n","\n","        return alphas, context_vector\n","\n","\n","class Decoder_LSTM(nn.Module):\n","    def __init__(self, embed_size, pretrained_embed, vocab_size, attention_dim, encoder_dim, decoder_dim):\n","        super().__init__()\n","\n","        # check whether to use pretrained word embedding or not\n","        if pretrained_embed is None:\n","            embed_size = embed_size\n","            self.embedding = nn.Embedding(vocab_size, embed_size)\n","        else:\n","            embed_size = pretrained_embed.shape[1]\n","            self.embedding = nn.Embedding.from_pretrained(pretrained_embed, freeze=True)\n","\n","        # set the model attributes\n","        self.vocab_size = vocab_size\n","        self.attention_dim = attention_dim\n","        self.decoder_dim = decoder_dim\n","\n","        #create attention object\n","        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)\n","\n","        #create initial hidden and cell state\n","        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n","        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n","        #create LSTMCell object for decoder\n","        self.lstm_cell = nn.LSTMCell(embed_size + encoder_dim, decoder_dim, bias=True)\n","\n","        #Linear transformation for attention output\n","        self.decoder_out = nn.Linear(decoder_dim, self.vocab_size)\n","        self.dropout = nn.Dropout(0.3)\n","\n","        #initialize model parameters randomely in range(-1, 1)\n","        self.embedding.weight.data.uniform_(-0.1, 0.1)\n","\n","    def forward(self, features, captions, caption_len):\n","        #sort the captions and images corresponding to their lenght\n","        caption_lengths, sorted_indices = caption_len.sort(dim=0, descending=True)\n","        features = features[sorted_indices,:,:] #[32, 49, 2048]\n","        captions = captions[sorted_indices,:]\n","\n","        seq_length = caption_lengths - 1 # Exclude the last one (the <end> position)\n","        batch_size = captions.size(0)\n","        num_features = features.size(1)\n","\n","        #from sequences of token IDs to sequences of word embeddings\n","        embeds = self.embedding(captions)\n","\n","        # Initialize LSTM hidden and cell states\n","        h, c = self.init_hidden_state(features)\n","\n","        #create zeros tensor for outputs and alphas\n","        outputs = torch.zeros(batch_size, seq_length[0], self.vocab_size).to(device)\n","        alphas = torch.zeros(batch_size, seq_length[0], num_features).to(device)\n","\n","        # for each time step we will ask attention model to returns a context vector\n","        # based on decoder's previous hidden state then we will generate new word\n","        for word in range(0, seq_length[0].item()):\n","\n","            #get context vector with the encoder features and previous hidden state\n","            alpha, context = self.attention(features, h) # context=[32, 2048]\n","\n","            #combine embedding vector of the word with context vector and feed it to lstmcell\n","            lstm_input = torch.cat([embeds[:, word], context], dim=1) # [32, 2348]\n","            h, c = self.lstm_cell(lstm_input, (h, c))\n","\n","            #get the logits of the decoder (also we used dropout for regularization purpose)\n","            logits = self.decoder_out(self.dropout(h))\n","\n","            #append all generated words in the outputs tensor\n","            outputs[:, word] = logits\n","            alphas[:, word] = alpha\n","\n","        return outputs, alphas, captions, seq_length\n","\n","    def CapGenerator(self, features, max_len=20, vocab=None):\n","        \"\"\" Method used to generate a caption for a given image \"\"\"\n"," \n","        alphas = []\n","        captions = []\n","        batch_size = features.size(0)\n","\n","        #generate initial hidden state\n","        h, c = self.init_hidden_state(features)\n","\n","        # create the initial sentence with starting token\n","        word = torch.tensor(vocab.vocabulary['<SOS>']).view(1, -1).to(device)\n","        embeds = self.embedding(word)\n","        \n","        #loop for iterating over the maximum sentence lenght to be generated\n","        for i in range(max_len):\n","\n","            #given the image to attention model it returns the context vector\n","            alpha, context = self.attention(features, h)\n","\n","            #storing the alphas score for loss function\n","            alphas.append(alpha.cpu().detach().numpy())\n","\n","            #generating the next word\n","            lstm_input = torch.cat((embeds[:, 0], context), dim=1)\n","            h, c = self.lstm_cell(lstm_input, (h, c))\n","            output = self.decoder_out(self.dropout(h))\n","            output = output.view(batch_size, -1)\n","\n","            # select the word with highest value\n","            index_of_predicted_word = output.argmax(dim=1)\n","\n","            # save the generated word into a list\n","            captions.append(index_of_predicted_word.item())\n","\n","            # check to stop generation if it predicted <EOS>\n","            if index_of_predicted_word.item() == 2: # 2 is the index of <EOS> in the vocabulary\n","                break\n","\n","            # send back the generated word as the next caption\n","            embeds = self.embedding(index_of_predicted_word.unsqueeze(0))\n","\n","        # covert the index of tokens into words\n","        return [vocab.get_key(idx) for idx in captions], alphas\n","\n","    def init_hidden_state(self, encoder_out):\n","        \"\"\" Method used for initial state for the models\n","        Args:\n","            encoder_out: this is the output of our encoder which we used it here to make an initial state,\n","                         it is a tensor of dimension (batch_size, num_pixels, encoder_dim)\n","        Return:\n","            h: hidden state with the dimension equal to decoder dimension\n","            c: cell state with output dimension size of decoder\n","        \"\"\"\n","        #get the mean of encoder output units\n","        mean_encoder_out = encoder_out.mean(dim=1)\n","\n","        #get the hidden and cell state by means of a linear transformation from encoder dim to decoder dim\n","        h = self.init_h(mean_encoder_out)\n","        c = self.init_c(mean_encoder_out)\n","\n","        return h, c\n","\n","class Encoder_Decoder(nn.Module):\n","    \"\"\" class that create the main model \"\"\"\n","    def __init__(self, embed_size, pretrained_embed, vocab_size, attention_dim, encoder_dim, decoder_dim):\n","        super().__init__()\n","\n","        self.encoder = Encoder_CNN()\n","        self.decoder = Decoder_LSTM(\n","            embed_size=embed_size,\n","            pretrained_embed = pretrained_embed,\n","            vocab_size=vocab_size,\n","            attention_dim=attention_dim,\n","            encoder_dim=encoder_dim,\n","            decoder_dim=decoder_dim, \n","            )\n","        #define the loss function to be used\n","        self.criterion = nn.CrossEntropyLoss()\n","        self.train_accuracies = None\n","        self.valid_accuracies = None\n","\n","\n","    def forward(self, images, captions, caption_len):\n","\n","        #feed the images to the encoder in order to get the features vector\n","        features = self.encoder(images)\n","\n","        #feed images and captions with their lenght to the decoder\n","        outputs, alphas, captions, seq_length = self.decoder(features, captions, caption_len)\n","\n","        return outputs, alphas, captions, seq_length\n","\n","\n","    def Train_model(self, train_set, valid_set, num_epochs, learning_rate, resume=None):\n","        \"\"\"main method used to train the network\"\"\" \n","        \n","        #set the optimizer\n","        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","        #ensuring that the model is on training mode\n","        model.train()\n","\n","        #check if we are going to resume the previously training state or not\n","        if resume:\n","            print(\"Resuming last model...\")\n","            start_epoch = model.resume('attention_model_state.pth', optimizer)\n","        else:\n","            start_epoch=0\n","\n","        #initialize some parameterz to be used during training\n","        vocab_size = len(_data_set.vocab)\n","        epoch_loss = 100\n","        epoch_acc = 0\n","        best_val_acc = -1 # the best accuracy computed on the validation data\n","        self.train_accuracies = np.zeros(num_epochs)\n","        self.valid_accuracies = np.zeros(num_epochs)\n","    \n","        #loop over the epoches\n","        for epoch in range(start_epoch, num_epochs):\n","\n","            train_tot_acc = 0 #total accuracy computed on training set\n","            train_tot_loss = 0 #total loss computed on training set\n","            num_batch = 0\n","\n","            for idx, (image, captions, caption_len) in enumerate(iter(train_set)):\n","    \n","                image, captions, caption_len = image.to(device), captions.to(device), caption_len.to(device)\n","    \n","                # Zero the gradients.\n","                optimizer.zero_grad()\n","    \n","                # Feed forward the data to the main model\n","                outputs, alphas, captions, seq_length = model(image, captions, caption_len)\n","                targets = captions[:, 1:] #skip the start token (<SOS>)\n","                \n","                #skip the padded sequences\n","                outputs = pack_padded_sequence(outputs, seq_length.cpu().numpy(), batch_first=True)\n","                targets= pack_padded_sequence(targets, seq_length.cpu().numpy(), batch_first=True)\n","\n","                #compute the accuracy of the model\n","                acc = model.__performance(outputs, targets)\n","\n","                #compute the loss\n","                loss = model.__loss(outputs, targets)\n","\n","                #try to minimize the difference between 1 and the sum of a pixel's weights across all timesteps\n","                loss += 1. * ((1. - alphas.sum(dim=1)) ** 2).mean()\n","\n","                #compute backward.\n","                loss.backward()\n","    \n","                #Update the parameters.\n","                optimizer.step()\n","    \n","                print(\"-train-minibatch: {} loss: {:.5f}\".format(idx+1, loss.item()))\n","                train_tot_acc += acc\n","                train_tot_loss += loss\n","                num_batch += 1\n","\n","            #try to plot an image with its corresponding generated captions on training set\n","            model.Test_and_Plot(train_set)\n","\n","            #compute trainin loss and accuracy (average)\n","            train_avg_acc = train_tot_acc/num_batch\n","            train_avg_loss = train_tot_loss/num_batch\n","            print(\"Average ---> acc: {:.2f} loss: {:.5f}\".format(train_avg_acc, train_avg_loss))\n","            print(\"---------------------------------------------\")\n","\n","            #evaluate the current model on validation set \n","            valid_avg_acc, valid_avg_loss = model.Evaluate_model(valid_set)\n","            print(\"Average ---> acc: {:.2f} loss: {:.5f}\".format(valid_avg_acc, valid_avg_loss))\n","            print(\"---------------------------------------------\")\n","\n","            #save the current model if it is the best\n","            if valid_avg_acc > best_val_acc:\n","                best_val_acc = valid_avg_acc\n","                best_epoch = epoch+1\n","                model.save(model, optimizer, epoch)\n","\n","            self.train_accuracies[epoch] = train_avg_acc\n","            self.valid_accuracies[epoch] = valid_avg_acc\n","\n","            print((\"Epoch={}/{}:  Tr_loss:{:.5f}  Tr_acc:{:.2f}  Va_acc:{:.2f}\" + \n","                   (\" ---> **BEST**\" if best_epoch == epoch + 1 else \"\"))\n","                    .format(epoch+1, num_epochs, train_avg_loss, train_avg_acc, valid_avg_acc))\n","            print(\"---------------------------------------------\")\n","\n","        #save the final model\n","        torch.save(model, 'attention_model.pth')\n","    \n","\n","    def Evaluate_model(self, valid_set):\n","\n","        #set the model on evaluating mode\n","        model.eval()\n","\n","        #set some initial parameteres\n","        tot_acc = 0\n","        tot_loss = 0\n","        num_batch = 0\n","    \n","        for idx, (image, captions, caption_len) in enumerate(iter(valid_set)):\n","            image, captions, caption_len = image.to(device), captions.to(device), caption_len.to(device)\n","\n","            #call the main model to generate the captions\n","            outputs, alphas, captions, seq_length = model(image, captions, caption_len)\n","            targets = captions[:, 1:] #skip the first token (SOS)\n","\n","            #skip the padded sequences\n","            outputs = pack_padded_sequence(outputs, seq_length.cpu().numpy(), batch_first=True)\n","            targets = pack_padded_sequence(targets, seq_length.cpu().numpy(), batch_first=True)\n","\n","            #compute the accuracy and loss\n","            acc = model.__performance(outputs, targets)\n","            loss = model.__loss(outputs, targets)\n","            loss += 1. * ((1. - alphas.sum(dim=1)) ** 2).mean()\n","\n","            #update parameters used during evaluation\n","            tot_acc += acc\n","            tot_loss += loss\n","            num_batch +=1\n","            print(\"-valid-minibatch: {} loss: {:.5f}\".format(idx+1, loss.item()))\n","\n","        #generate a caption for an image from validation set to show the accuracy\n","        model.Test_and_Plot(valid_set)\n","\n","        #compute the accuracy and loss of validation set (average)\n","        avg_acc = tot_acc/num_batch\n","        avg_loss = tot_loss/num_batch\n","        model.train() #set the model back to training mode\n","    \n","        return avg_acc, avg_loss\n","\n","\n","    def Test_and_Plot(self, test_data, attention=None):\n","        \"\"\"Method used to plot the image with its corresponding generated caption \"\"\"\n","\n","        model.eval()\n","\n","        with torch.no_grad():\n","            dataiter = iter(test_data)\n","            img, _, _ = next(dataiter)\n","            features = model.encoder(img[0:1].to(device))\n","            caps, alphas = model.decoder.CapGenerator(features, vocab=_data_set.vocab)\n","            caption = ' '.join(caps)\n","            show_image(img[0], title=caption)\n","\n","            if attention:\n","                plot_attention(img[0], caps, alphas)\n","\n","\n","    def __loss(self, outputs, targets):\n","        \"\"\" function to be used for computing the loss \"\"\" \n","\n","        loss = self.criterion(outputs.data, targets.data)\n","        return loss\n","\n","\n","    def __performance(self, outputs, targets):\n","        \"\"\"function to be used for computing the performance of the model \"\"\" \n","\n","        #returns the index of the word with the highst value\n","        highest_indices = outputs.data.argmax(dim=1)\n","        highest_indices = highest_indices.reshape(-1, 1)\n","\n","        #check if the predicted output is equal to the targets\n","        word_correct = highest_indices.eq(targets.data.view(-1,1))\n","        seq_correct = word_correct.float().sum()\n","\n","        #compute the batch accuracy\n","        acc = seq_correct.item() * (100.0/targets.data.shape[0])\n","\n","        return acc\n","    \n","\n","    def save(self, model, optimizer, num_epochs):\n","        \"\"\"save the model\"\"\"\n","        checkpoint = {\n","            'num_epochs':num_epochs,\n","            'optimizer': optimizer.state_dict(),\n","            'model_state':model.state_dict()\n","            }\n","        torch.save(checkpoint,'attention_model_state.pth')\n","        return\n","    \n","    def resume(self, checkpoint, optimizer):\n","        \"\"\"resume the model\"\"\"\n","        checkpoint = torch.load(checkpoint, map_location=device)\n","        start_epoch = checkpoint['num_epochs']\n","        model.load_state_dict(checkpoint['model_state'])\n","        optimizer.load_state_dict(checkpoint['optimizer'])\n","        return start_epoch\n","\n","\n","def plot_attention(img, result, attention_plot):\n","    # recover the original image from transformed image\n","    img[0] = img[0] * 0.229\n","    img[1] = img[1] * 0.224\n","    img[2] = img[2] * 0.225\n","    img[0] += 0.485\n","    img[1] += 0.456\n","    img[2] += 0.406\n","\n","    img = img.numpy().transpose((1, 2, 0))\n","    temp_image = img\n","\n","    fig = plt.figure(figsize=(15, 15))\n","\n","    len_result = len(result)\n","    for l in range(len_result):\n","        temp_att = attention_plot[l].reshape(7, 7)\n","\n","        ax = fig.add_subplot(len_result // 2, len_result // 2, l + 1)\n","        ax.set_title(result[l])\n","        img = ax.imshow(temp_image)\n","        ax.imshow(temp_att, cmap='gray', alpha=0.7, extent=img.get_extent())\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","def show_image(img, title=None):\n","    \"\"\"Imshow for Tensor.\"\"\"\n","    # unnormalize\n","    img[0] = img[0] * 0.229\n","    img[1] = img[1] * 0.224\n","    img[2] = img[2] * 0.225\n","    img[0] += 0.485\n","    img[1] += 0.456\n","    img[2] += 0.406\n","    img = img.numpy().transpose((1, 2, 0))\n","    plt.imshow(img)\n","    if title is not None:\n","        plt.title(title)\n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","\n","def save_acc_graph(train_accs, valid_accs):\n","    \"\"\"Plot the accuracies of the training and validation data computed during the training stage.\n","\n","    Args:\n","        train_accs,valid_accs: the arrays with the training and validation accuracies (same length).\n","    \"\"\"\n","\n","    plt.figure().clear()\n","    plt.clf()\n","    plt.close()\n","    plt.figure()\n","    plt.plot(train_accs, label='Training Data')\n","    plt.plot(valid_accs, label='Validation Data')\n","    plt.ylabel('Accuracy %')\n","    plt.xlabel('Epochs')\n","    plt.ylim((0, 100))\n","    plt.legend(loc='lower right')\n","    plt.savefig('training_stage.pdf')\n","    plt.figure().clear()\n","    plt.close()\n","    plt.clf()\n","\n","\n","def parse_command_line_arguments():\n","    \"\"\"Parse command line arguments and checking their values\"\"\"\n","\n","    parser = argparse.ArgumentParser(description='')\n","\n","    parser.add_argument('mode', type=str, choices=['train', 'eval', 'test'],\n","                        help='train or evaluate or test the model')\n","    parser.add_argument('--resume', type=str, default=None,\n","                        help='resume from previouse training phase( 1:Yes or 0:No) default: 0')\n","    parser.add_argument('data_location', type=str,\n","                        help='define training_set or test_set directory')\n","    parser.add_argument('--batch_size', type=int, default=32,\n","                        help='mini-batch size (default: 32)')\n","    parser.add_argument('--epochs', type=int, default=50,\n","                        help='number of training epochs (default: 10)')\n","    parser.add_argument('--learning_rate', type=float, default=3e-4,\n","                        help='learning rate (Adam) (default: 3e-4)')\n","    parser.add_argument('--workers', type=int, default=1,\n","                        help='number of working units used to load the data (default: 0)')\n","    parser.add_argument('--freq_threshold', type=int, default=3,\n","                        help='threshold for word frequencies (default: 1)')\n","    parser.add_argument('--randomize', type=str, default=None,\n","                        help='shuffling the data set before splitting (1:Yes or 0:No) default: 1 ')\n","    parser.add_argument('--preprocess', type=str, default=None,\n","                        help='choose a customize preprocess {default or custom} default: default ')\n","    parser.add_argument('--splits', type=str, default='0.04-0.008-0.008',\n","                        help='fraction of data to be used in train set and val set (default: 0.7-0.3)')\n","    parser.add_argument('--glove_embeddings', type=str, default=None,\n","                        help='pre-trained embeddings file will be loaded (default: None)')  \n","    parser.add_argument('--embed_size', type=int, default=256,\n","                        help='word embedding size (default: 128)')\n","    parser.add_argument('--attention_dim', type=int, default=256,\n","                        help='input dimension of attention model (default: 256)')\n","    parser.add_argument('--encoder_dim', type=int, default=2048,\n","                        help='input dimension of encoder model (default: 2048)')\n","    parser.add_argument('--decoder_dim', type=int, default=512,\n","                        help='input dimension of decoder model (default: 512)')\n","    parser.add_argument('--device', default='gpu', type=str,\n","                        help='device to be used for computations {cpu, gpu} default: gpu')\n","\n","    parsed_arguments = parser.parse_args(['train', '/content/drive/MyDrive/Image Caption with Attention/flickr8k'])\n","\n","    #converting split fraction string to a list of floating point values ('0.7-0.15-0.15' => [0.7, 0.15, 0.15])\n","    splits_string = str(parsed_arguments.splits)\n","    fractions_string = splits_string.split('-')\n","    if len(fractions_string) != 3:\n","        raise ValueError(\"Invalid split fractions were provided. Required format (example): 0.7-0.15-0.15\")\n","    else:\n","        splits = []\n","        frac_sum = 0.\n","        for fraction in fractions_string:\n","            try:\n","                splits.append(float(fraction))\n","                frac_sum += splits[-1]\n","            except ValueError:\n","                raise ValueError(\"Invalid split fractions were provided. Required format (example): 0.7-0.15-0.15\")\n","        if frac_sum > 1.0 or frac_sum < 0.0:\n","            raise ValueError(\"Invalid split fractions were provided. They must sum to 1.\")\n","\n","    # updating the 'splits' argument\n","    parsed_arguments.splits = splits\n","\n","    return parsed_arguments\n","\n","\n","\n","\"\"\" STARTING FROM HERE \"\"\"\n","\n","if __name__ == \"__main__\":\n","    args = parse_command_line_arguments()\n","\n","    print(\"\\n-------------------------------\")\n","    for k, v in args.__dict__.items():\n","        print(k + '=' + str(v))\n","\n","\n","    # *** TRAINING *** \n","    # if you have choosed 'train' then it will start to train the model from here.\n","\n","    print(\"-------------------------------\")\n","    if args.mode == 'train':\n","\n","        if args.device == 'gpu':\n","            if torch.cuda.is_available():\n","                device = torch.device(\"cuda\")\n","                print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n","                print('Using device:', torch.cuda.get_device_name(0))\n","            else:\n","                print('GPU is not availabe!! Using device: CPU')\n","                device = torch.device(\"cpu\")\n","        else:\n","            print('Using device: CPU')\n","            device = torch.device(\"cpu\")\n","        \n","        #create the dataset object\n","        _data_set = CapDataset(path=args.data_location + \"/Images\",\n","                                  captions_file=args.data_location + \"/captions.txt\",\n","                                  preprocess_custom=args.preprocess)\n","\n","        #check if you need to randomize the data or not\n","        if args.randomize:\n","            _data_set.randomize_data()\n","\n","        #split the data into three parts:\n","        [_train_set, _val_set, _test_set] = _data_set.create_splits(args.splits)\n","\n","        #check whether using pretrained embeddings or not\n","        if args.glove_embeddings is not None:\n","            _pretrained_embed, embed_size = _data_set.vocab.load_embedding_file(args.glove_embeddings)\n","            print(\"Loading embeddings: DONE\")\n","            print(\"Embeddding_size= \", embed_size)\n","            print(\"-------------------------------\")\n","        else:\n","            _pretrained_embed = None\n","\n","        #create dataloader for training set\n","        _train_set = DataLoader(dataset=_train_set, batch_size=args.batch_size, num_workers=args.workers, shuffle=True,\n","                                        collate_fn=_data_set.CapCollate)\n","        #create dataloader for validation set\n","        _val_set   = DataLoader(dataset=_val_set, batch_size=args.batch_size, num_workers=args.workers, shuffle=True,\n","                                        collate_fn=_data_set.CapCollate)\n","        #create dataloader for test set\n","        _test_set  = DataLoader(dataset=_test_set, batch_size=args.batch_size, num_workers=args.workers, shuffle=True,\n","                                        collate_fn=_data_set.CapCollate)\n","\n","        #create the model\n","        model = Encoder_Decoder( embed_size=args.embed_size,\n","                                pretrained_embed = _pretrained_embed,\n","                                vocab_size=len(_data_set.vocab),\n","                                attention_dim=args.attention_dim,\n","                                encoder_dim=args.encoder_dim,\n","                                decoder_dim=args.decoder_dim,\n","                               ).to(device)\n","\n","        #starting to train the model\n","        print(\"-------------------------------\")\n","        print(\"\\nTraining The Model...\")\n","        model.Train_model(_train_set, _val_set, args.epochs, args.learning_rate, args.resume)\n","        save_acc_graph(model.train_accuracies, model.valid_accuracies)\n","\n","        #starting to evaluate the model (on trainin-set, validation-set, test-set)\n","        print(\"-------------------------------\")\n","        print(\"\\nEvaluating The Model...\")\n","\n","        print(\"\\n-On training_set...\")\n","        train_acc, train_loss = model.Evaluate_model(_train_set)\n","        print(\"Average On Training ---> acc:{:.2f}  loss:{:.5f}\".format(train_acc, train_loss))\n","\n","        print(\"\\n-On validation_set...\")\n","        val_acc, val_loss = model.Evaluate_model(_val_set)\n","        print(\"Average On Validation ---> acc:{:.2f}  loss:{:.5f}\".format(val_acc, val_loss))\n","\n","        print(\"\\n-On test_set...\")\n","        test_acc, test_loss = model.Evaluate_model(_test_set)\n","        print(\"Average On Test ---> acc:{:.2f}  loss:{:.5f}\".format(test_acc, test_loss))\n","\n","\n","    # *** EVALUATE *** \n","    # if you have choosed 'test' then it will evaluate the model starting from here:\n","\n","    elif args.mode == 'eval':\n","        print(\"\\nEvaluating The Model...\")\n","\n","        if args.device == 'gpu':\n","            if torch.cuda.is_available():\n","                device = torch.device(\"cuda\")\n","                print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n","                print('Using device::', torch.cuda.get_device_name(0))\n","            else:\n","                print('GPU is not availabe!! Using device: CPU')\n","                device = torch.device(\"cpu\")\n","        else:\n","            print('Using device: CPU')\n","            device = torch.device(\"cpu\")\n","\n","        #create the dataset object\n","        _data_set = CapDataset(path=args.data_location + \"/Images\",\n","                                  captions_file=args.data_location + \"/captions.txt\",\n","                                   empty_dataset=True, eval=True)\n","\n","        #create dataloader for the data set to be validate the model\n","        _val_set  = DataLoader(dataset=_data_set, batch_size=args.batch_size, num_workers=args.workers, shuffle=False,\n","                                        collate_fn=_data_set.CapCollate)\n","\n","        # check whether using pretrained embeddings or not\n","        if args.glove_embeddings is not None:\n","            _pretrained_embed, embed_size = _data_set.vocab.load_embedding_file(args.glove_embeddings)\n","            print(\"Loading embeddings: DONE\")\n","            print(\"Embeddding_size= \", embed_size)\n","            print(\"-------------------------------\")\n","        else:\n","            _pretrained_embed = None\n","\n","        #create the initial model\n","        model = Encoder_Decoder( embed_size=args.embed_size,\n","                                pretrained_embed = _pretrained_embed,\n","                                vocab_size=len(_data_set.vocab),\n","                                attention_dim=args.attention_dim,\n","                                encoder_dim=args.encoder_dim,\n","                                decoder_dim=args.decoder_dim\n","                               ).to(device)\n","\n","        #check the path to load the model in which you want to evaluate it\n","        if not os.path.exists('attention_model.pth'):\n","            raise ValueError(\"There is no 'attention_model.pth' file!!!\")\n","        else:\n","            print(\"-------------------------------\")\n","            print(\"loading the model...\")\n","\n","        #load the model\n","        model = torch.load('attention_model.pth', map_location=device)\n","\n","        #start to evaluate the model with the corresponding dataset\n","        val_acc, val_loss = model.Evaluate_model(_val_set)\n","        print(\"Average On Validation ---> acc:{:.2f}  loss:{:.5f}\".format(val_acc, val_loss))\n","\n","\n","    # *** TEST *** \n","    # If you have choosed 'test' then it will only test the model starting from here.\n","\n","    elif args.mode == 'test':\n","        print(\"\\nTesting The Model...\")\n","\n","        if args.device == 'gpu':\n","            if torch.cuda.is_available():\n","                device = torch.device(\"cuda\")\n","                print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n","                print('Using device:', torch.cuda.get_device_name(0))\n","            else:\n","                print('GPU is not availabe!! Using device: CPU')\n","                device = torch.device(\"cpu\")\n","        else:\n","            print('Using device: CPU')\n","            device = torch.device(\"cpu\")\n","\n","        #create the dataset object \n","        _data_set = CapDataset(path=args.data_location + \"/Images\",\n","                                  captions_file=args.data_location + \"/captions.txt\",\n","                                   empty_dataset=True, test=True)\n","\n","        #create dataloader for the test set\n","        _test_dataset  = DataLoader(dataset=_data_set, batch_size=1, num_workers=1, shuffle=False,\n","                                            collate_fn=_data_set.CapCollate)\n","\n","        # check whether using pretrained embeddings or not\n","        if args.glove_embeddings is not None:\n","            _pretrained_embed, embed_size = _data_set.vocab.load_embedding_file(args.glove_embeddings)\n","            print(\"Loading embeddings: DONE\")\n","            print(\"Embeddding_size= \", embed_size)\n","            print(\"-------------------------------\")\n","        else:\n","            _pretrained_embed = None\n","\n","        #create the model object\n","        model = Encoder_Decoder( embed_size=args.embed_size,\n","                                pretrained_embed = _pretrained_embed,\n","                                vocab_size=len(_data_set.vocab),\n","                                attention_dim=args.attention_dim,\n","                                encoder_dim=args.encoder_dim,\n","                                decoder_dim=args.decoder_dim\n","                               ).to(device)\n","        #check whether the model exists in the path or not\n","        if not os.path.exists('attention_model.pth'):\n","            raise ValueError(\"There is no 'attention_model.pth' file!!!\")\n","        print(\"-------------------------------\")\n","        model = torch.load('attention_model.pth', map_location=device)\n","\n","        #set the model to be in evaluation mode\n","        model.eval()\n","\n","        #iterate over the test images that you provided\n","        dataiter = iter(_test_dataset)\n","        for i in range(0, len(_data_set)):\n","            model.Test_and_Plot(dataiter, attention=True)\n","\n","\n","    else:\n","        raise ValueError(\"You must specify the operation you need!!! ('train', 'eval', 'test'\")\n","\n","\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["\n","-------------------------------\n","mode=train\n","resume=None\n","data_location=/content/drive/MyDrive/Image Caption with Attention/flickr8k\n","batch_size=32\n","epochs=50\n","learning_rate=0.0003\n","workers=1\n","freq_threshold=3\n","randomize=None\n","preprocess=None\n","splits=[0.04, 0.008, 0.008]\n","glove_embeddings=None\n","embed_size=256\n","attention_dim=256\n","encoder_dim=2048\n","decoder_dim=512\n","device=gpu\n","-------------------------------\n","GPU is not availabe!! Using device: CPU\n","Vocabulary size:  4093\n","Dataset size: 40455\n","Selected Dataset size: 2264\n","-Training size: 1618\n","-Validation size: 323\n","-Test size: 323\n","-------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["Downloading: \"https://github.com/pytorch/vision/archive/v0.7.0.zip\" to /root/.cache/torch/hub/v0.7.0.zip\n","Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f1e92cacf54742ad8256c6191b8bcc6c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","-------------------------------\n","\n","Training The Model...\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-55880d1891ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTraining The Model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_train_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_val_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m         \u001b[0msave_acc_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_accuracies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-55880d1891ee>\u001b[0m in \u001b[0;36mTrain_model\u001b[0;34m(self, train_set, valid_set, num_epochs, learning_rate, resume)\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0mnum_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption_len\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m                 \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption_len\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}